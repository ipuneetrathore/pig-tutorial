{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_Finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0e0c9ce1a1a41eba222c5789119f7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ded08bd0e4704a3488cf7c3d48056fde",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f255e29de322488bbc835e2838530a4a",
              "IPY_MODEL_82dff7f7a5744eed83d82b991ae8270b"
            ]
          }
        },
        "ded08bd0e4704a3488cf7c3d48056fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "f255e29de322488bbc835e2838530a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7000b9c97e19472181d3aa6358a76975",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_add4e2fbf2ea4957bd4ccc997c5b16a4"
          }
        },
        "82dff7f7a5744eed83d82b991ae8270b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01088b8a732148da9bbfc8c1e7c633b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:02&lt;00:00,  2.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44f1c7aa65eb410bad523218e78bcd02"
          }
        },
        "7000b9c97e19472181d3aa6358a76975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "add4e2fbf2ea4957bd4ccc997c5b16a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01088b8a732148da9bbfc8c1e7c633b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44f1c7aa65eb410bad523218e78bcd02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxDzTIn15Ou5"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo8Qw1rS5Obk",
        "outputId": "f92c03c3-5cf6-4140-830e-5d1c203462ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqkOSoBd5UMN",
        "outputId": "ce88b50a-696b-4f37-b052-779deee45a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/totto_data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "totto_dev_data.jsonl  totto_train_data.jsonl  unlabeled_totto_test_data.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlnvO7_q5e94"
      },
      "source": [
        "!cp \"/content/drive/My Drive/totto_data/totto_train_data.jsonl\" -r \"totto\"\n",
        "!cp \"/content/drive/My Drive/totto_data/totto_dev_data.jsonl\" -r \"totto\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrL-6dvSqgh_"
      },
      "source": [
        "!pip install pytorch_lightning=='0.7.5'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYNOVtGEsUvJ"
      },
      "source": [
        "!pip install transformers=='2.9.0'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "herz3gF6e7yb"
      },
      "source": [
        "# Import libraries\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrXpnJW-BwoS",
        "outputId": "50cd0026-ba03-46f0-e509-a5d4104af488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.6.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVzX9GMyo41p"
      },
      "source": [
        "import copy\n",
        "import json\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import pandas as pd\n",
        "import six"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUDJxCado496"
      },
      "source": [
        "trainDataLst = []\n",
        "\n",
        "with open('totto/totto_train_data.jsonl', 'r') as fd:\n",
        "  for l in fd:\n",
        "    trainDataLst.append(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sV-Pu4c2lV6"
      },
      "source": [
        "valDataLst = []\n",
        "\n",
        "with open('totto/totto_dev_data.jsonl', 'r') as fd:\n",
        "  for l in fd:\n",
        "    valDataLst.append(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB_5067ae91i",
        "outputId": "91019ed5-05e9-49a0-f60f-2cc78344150e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(trainDataLst), len(valDataLst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(81710, 7700)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC6A34ITHiL0",
        "outputId": "c411e944-914a-4eae-f92f-894d53e3492d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "['table', 'table_webpage_url', 'table_page_title', 'table_section_title', 'table_section_text', \n",
        " 'highlighted_cells', 'example_id', 'sentence_annotations']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['table',\n",
              " 'table_webpage_url',\n",
              " 'table_page_title',\n",
              " 'table_section_title',\n",
              " 'table_section_text',\n",
              " 'highlighted_cells',\n",
              " 'example_id',\n",
              " 'sentence_annotations']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_hIu35lS6th"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isrVp9UDyoYt"
      },
      "source": [
        "def get_highligted_data(trainInput):\n",
        "\n",
        "  highCellVals = []\n",
        "  #table_headers = [h['value'] for h in trainInput['table'][0]]\n",
        "  table_page_title = ['table_page_title',trainInput['table_page_title']]\n",
        "  highCellVals.append(table_page_title)\n",
        "  table_section_title = ['table_section_title',trainInput['table_section_title']]\n",
        "  highCellVals.append(table_section_title)\n",
        "  table_section_text = ['table_section_text',trainInput['table_section_text']]\n",
        "  highCellVals.append(table_section_text)\n",
        "  for hc in trainInput['highlighted_cells']:\n",
        "    highCellVals.append([trainInput['table'][hc[0]][hc[1]]['value']])\n",
        "\n",
        "  tar_txt = ['final_sentence',trainInput['sentence_annotations'][0]['final_sentence']]\n",
        "  highCellVals.append(tar_txt)\n",
        "\n",
        "  return highCellVals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OkUNLI_ONPw"
      },
      "source": [
        "trainDataPrep1 = []\n",
        "\n",
        "for i,entry in enumerate(trainDataLst[:-1]):\n",
        "  print(i)\n",
        "  trainDataPrep1.append(get_highligted_data(json.loads(entry)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q08hc9rzynMI",
        "outputId": "cccb8193-48cf-4a34-cd7b-313b1db75d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "trainDataPrep1[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['table_page_title', 'List of Chicago Bears first-round draft picks'],\n",
              " ['table_section_title', 'Player selections'],\n",
              " ['table_section_text', ''],\n",
              " ['2018'],\n",
              " ['Roquan Smith'],\n",
              " ['Linebacker'],\n",
              " ['Georgia'],\n",
              " ['final_sentence',\n",
              "  'The Chicago Bears recent first round selection (2018) was Roquan Smith, an inside linebacker from Georgia.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAQ87rdDynFr"
      },
      "source": [
        "## CREATE TRAINING DATA\n",
        "trainDataLst = []\n",
        "prefix = 'sent_generator'\n",
        "for tde in trainDataPrep1:\n",
        "  param_lst = []\n",
        "  for in_ent in tde[:-1]:\n",
        "    if in_ent[-1] != '':\n",
        "      param_lst.append(in_ent[-1])\n",
        "  trainDataLst.append([prefix, '&&'.join(param_lst), tde[-1][-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75kRh3qkym_a"
      },
      "source": [
        "#trainDF = pd.DataFrame(trainDataLst, columns = ['prefix','input','target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeudS_m6ym4g"
      },
      "source": [
        "#trainDF.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVznpa_x2f-h"
      },
      "source": [
        "valDataPrep1 = []\n",
        "\n",
        "for i,entry in enumerate(valDataLst[:-1]):\n",
        "  print(i)\n",
        "  valDataPrep1.append(get_highligted_data(json.loads(entry)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdh0nPQW2gIs"
      },
      "source": [
        "# CREATE TRAINING DATA\n",
        "valDataLst = []\n",
        "prefix = 'sent_generator'\n",
        "for tde in valDataPrep1:\n",
        "  param_lst = []\n",
        "  for in_ent in tde[:-1]:\n",
        "    if in_ent[-1] != '':\n",
        "      param_lst.append(in_ent[-1])\n",
        "  valDataLst.append([prefix, '&&'.join(param_lst), tde[-1][-1]])\n",
        "\n",
        "#valDF = pd.DataFrame(valDataLst, columns = ['prefix','input','target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQDoMh932gFR"
      },
      "source": [
        "#valDF.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr9fxlnOe9tT"
      },
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPLWtzSee9m7"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "  def __init__(self, hparams):\n",
        "    super(T5FineTuner, self).__init__()\n",
        "    self.hparams = hparams\n",
        "    \n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "  \n",
        "  def is_logger(self):\n",
        "    return self.trainer.proc_rank <= 0\n",
        "  \n",
        "  def forward(\n",
        "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
        "  ):\n",
        "    return self.model(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        decoder_attention_mask=decoder_attention_mask,\n",
        "        lm_labels=lm_labels,\n",
        "    )\n",
        "\n",
        "  def _step(self, batch):\n",
        "    lm_labels = batch[\"target_ids\"]\n",
        "    lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    outputs = self(\n",
        "        input_ids=batch[\"source_ids\"],\n",
        "        attention_mask=batch[\"source_mask\"],\n",
        "        lm_labels=lm_labels,\n",
        "        decoder_attention_mask=batch['target_mask']\n",
        "    )\n",
        "\n",
        "    loss = outputs[0]\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    loss = self._step(batch)\n",
        "\n",
        "    tensorboard_logs = {\"train_loss\": loss}\n",
        "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "  \n",
        "  def training_epoch_end(self, outputs):\n",
        "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
        "    return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    loss = self._step(batch)\n",
        "    return {\"val_loss\": loss}\n",
        "  \n",
        "  def validation_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "    return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "    model = self.model\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": self.hparams.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "    self.opt = optimizer\n",
        "    return [optimizer]\n",
        "  \n",
        "  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
        "    if self.trainer.use_tpu:\n",
        "      xm.optimizer_step(optimizer)\n",
        "    else:\n",
        "      optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    self.lr_scheduler.step()\n",
        "  \n",
        "  def get_tqdm_dict(self):\n",
        "    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "    return tqdm_dict\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
        "    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
        "    t_total = (\n",
        "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "        // self.hparams.gradient_accumulation_steps\n",
        "        * float(self.hparams.num_train_epochs)\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "    self.lr_scheduler = scheduler\n",
        "    return dataloader\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
        "    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FskOA8Cp1yIZ"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "  def on_validation_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Validation results *****\")\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "      # Log results\n",
        "      for key in sorted(metrics):\n",
        "        if key not in [\"log\", \"progress_bar\"]:\n",
        "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "  def on_test_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Test results *****\")\n",
        "\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "\n",
        "      # Log and save results to file\n",
        "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "      with open(output_test_results_file, \"w\") as writer:\n",
        "        for key in sorted(metrics):\n",
        "          if key not in [\"log\", \"progress_bar\"]:\n",
        "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8s1Ansi1yjW"
      },
      "source": [
        "args_dict = dict(\n",
        "    data_dir=\"\", # path for data files\n",
        "    output_dir= \"\",  # path to save the checkpoints\n",
        "    model_name_or_path='t5-base',\n",
        "    tokenizer_name_or_path='t5-base',\n",
        "    max_seq_length=512,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=2,\n",
        "    eval_batch_size=2,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    n_gpu=1,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIUyWC7J1yeM"
      },
      "source": [
        "class tottoDataset(Dataset):\n",
        "  def __init__(self, tokenizer, dataLst, type_path,  max_len=512):\n",
        "    #self.pos_file_path = os.path.join(data_dir, type_path, 'pos')\n",
        "    #self.neg_file_path = os.path.join(data_dir, type_path, 'neg')\n",
        "    \n",
        "    #self.pos_files = glob.glob(\"%s/*.txt\" % self.pos_file_path)\n",
        "    #self.neg_files = glob.glob(\"%s/*.txt\" % self.neg_file_path)\n",
        "    \n",
        "    self.max_len = max_len\n",
        "    self.tokenizer = tokenizer\n",
        "    self.inputs = []\n",
        "    self.targets = []\n",
        "    self.dataLst = dataLst\n",
        "    self._build()\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.inputs)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "  \n",
        "  def _build(self):\n",
        "    self._buil_examples_from_files()\n",
        "\n",
        "  def _buil_examples_from_files(self):\n",
        "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "    for text in self.dataLst:\n",
        "      \n",
        "      line = text[1].strip()\n",
        "      #line = REPLACE_NO_SPACE.sub(\"\", line) \n",
        "      #line = REPLACE_WITH_SPACE.sub(\"\", line)\n",
        "      line = line + ' </s>'\n",
        "\n",
        "      target = text[2] + \" </s>\"\n",
        "      \n",
        "       # tokenize inputs\n",
        "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "          [line], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "      )\n",
        "       # tokenize targets\n",
        "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "          [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "      )\n",
        "\n",
        "      self.inputs.append(tokenized_inputs)\n",
        "      self.targets.append(tokenized_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge9_TApZ1yb0"
      },
      "source": [
        "# trainDF = pd.DataFrame(trainDataLst, columns = ['prefix','input','target'])\n",
        "# valDF = pd.DataFrame(valDataLst, columns = ['prefix','input','target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5wrfYzYr_Z_"
      },
      "source": [
        "#tokenizer = T5Tokenizer.from_pretrained('t5-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2C-N0JG1yaC"
      },
      "source": [
        "#dataset = tottoDataset(tokenizer, valDataLst, type_path='test')\n",
        "#len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5JZ4OAT1yVH"
      },
      "source": [
        "#len(dataset), len(valDataLst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjO2Fraj9wq9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE3sEhhYH4R2"
      },
      "source": [
        "# data = dataset[10]\n",
        "# print(tokenizer.decode(data['source_ids']))\n",
        "# print(tokenizer.decode(data['target_ids']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgxvkHeTH4Oe"
      },
      "source": [
        "!mkdir -p t5_totto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbRdjF5pvwiq"
      },
      "source": [
        "args_dict.update({'output_dir': 't5_totto', 'num_train_epochs':2})\n",
        "args = argparse.Namespace(**args_dict)\n",
        "\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
        ")\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    early_stop_callback=False,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7QGKK7nvwgj"
      },
      "source": [
        "def get_dataset(tokenizer, type_path, args):\n",
        "  return tottoDataset(tokenizer=tokenizer, dataLst=trainDataLst, type_path=type_path, max_len=args.max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnjU5NNdvwbl",
        "outputId": "933d9865-9121-47d6-e008-b06f26dc27ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        }
      },
      "source": [
        "model = T5FineTuner(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json from cache at /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n",
            "INFO:transformers.configuration_utils:Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/t5-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n",
            "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQeSK0pqvwaa",
        "outputId": "3b998519-0848-4ea5-de2e-3a7803ea3d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "trainer = pl.Trainer(**train_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:lightning:GPU available: True, used: True\n",
            "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsmJZerOvwXe",
        "outputId": "45b9c25f-6e66-48dc-ed82-b1cbab13129a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e0e0c9ce1a1a41eba222c5789119f7ca",
            "ded08bd0e4704a3488cf7c3d48056fde",
            "f255e29de322488bbc835e2838530a4a",
            "82dff7f7a5744eed83d82b991ae8270b",
            "7000b9c97e19472181d3aa6358a76975",
            "add4e2fbf2ea4957bd4ccc997c5b16a4",
            "01088b8a732148da9bbfc8c1e7c633b6",
            "44f1c7aa65eb410bad523218e78bcd02"
          ]
        }
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:lightning:\n",
            "    | Name                                                                  | Type                       | Params\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                                 | T5ForConditionalGeneration | 222 M \n",
            "1   | model.shared                                                          | Embedding                  | 24 M  \n",
            "2   | model.encoder                                                         | T5Stack                    | 109 M \n",
            "3   | model.encoder.block                                                   | ModuleList                 | 84 M  \n",
            "4   | model.encoder.block.0                                                 | T5Block                    | 7 M   \n",
            "5   | model.encoder.block.0.layer                                           | ModuleList                 | 7 M   \n",
            "6   | model.encoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "7   | model.encoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "8   | model.encoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "9   | model.encoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "10  | model.encoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "11  | model.encoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "12  | model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   \n",
            "13  | model.encoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "14  | model.encoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
            "15  | model.encoder.block.0.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "16  | model.encoder.block.0.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "17  | model.encoder.block.0.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "18  | model.encoder.block.0.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "19  | model.encoder.block.0.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "20  | model.encoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "21  | model.encoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
            "22  | model.encoder.block.1                                                 | T5Block                    | 7 M   \n",
            "23  | model.encoder.block.1.layer                                           | ModuleList                 | 7 M   \n",
            "24  | model.encoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "25  | model.encoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "26  | model.encoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "27  | model.encoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "28  | model.encoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "29  | model.encoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "30  | model.encoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "31  | model.encoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
            "32  | model.encoder.block.1.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "33  | model.encoder.block.1.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "34  | model.encoder.block.1.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "35  | model.encoder.block.1.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "36  | model.encoder.block.1.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "37  | model.encoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "38  | model.encoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
            "39  | model.encoder.block.2                                                 | T5Block                    | 7 M   \n",
            "40  | model.encoder.block.2.layer                                           | ModuleList                 | 7 M   \n",
            "41  | model.encoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "42  | model.encoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "43  | model.encoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "44  | model.encoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "45  | model.encoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "46  | model.encoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "47  | model.encoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "48  | model.encoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
            "49  | model.encoder.block.2.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "50  | model.encoder.block.2.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "51  | model.encoder.block.2.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "52  | model.encoder.block.2.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "53  | model.encoder.block.2.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "54  | model.encoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "55  | model.encoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
            "56  | model.encoder.block.3                                                 | T5Block                    | 7 M   \n",
            "57  | model.encoder.block.3.layer                                           | ModuleList                 | 7 M   \n",
            "58  | model.encoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "59  | model.encoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "60  | model.encoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "61  | model.encoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "62  | model.encoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "63  | model.encoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "64  | model.encoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "65  | model.encoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
            "66  | model.encoder.block.3.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "67  | model.encoder.block.3.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "68  | model.encoder.block.3.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "69  | model.encoder.block.3.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "70  | model.encoder.block.3.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "71  | model.encoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "72  | model.encoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
            "73  | model.encoder.block.4                                                 | T5Block                    | 7 M   \n",
            "74  | model.encoder.block.4.layer                                           | ModuleList                 | 7 M   \n",
            "75  | model.encoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "76  | model.encoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "77  | model.encoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "78  | model.encoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "79  | model.encoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "80  | model.encoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "81  | model.encoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "82  | model.encoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
            "83  | model.encoder.block.4.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "84  | model.encoder.block.4.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "85  | model.encoder.block.4.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "86  | model.encoder.block.4.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "87  | model.encoder.block.4.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "88  | model.encoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "89  | model.encoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
            "90  | model.encoder.block.5                                                 | T5Block                    | 7 M   \n",
            "91  | model.encoder.block.5.layer                                           | ModuleList                 | 7 M   \n",
            "92  | model.encoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "93  | model.encoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "94  | model.encoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "95  | model.encoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "96  | model.encoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "97  | model.encoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "98  | model.encoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "99  | model.encoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
            "100 | model.encoder.block.5.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "101 | model.encoder.block.5.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "102 | model.encoder.block.5.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "103 | model.encoder.block.5.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "104 | model.encoder.block.5.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "105 | model.encoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "106 | model.encoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
            "107 | model.encoder.block.6                                                 | T5Block                    | 7 M   \n",
            "108 | model.encoder.block.6.layer                                           | ModuleList                 | 7 M   \n",
            "109 | model.encoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "110 | model.encoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "111 | model.encoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "112 | model.encoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "113 | model.encoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "114 | model.encoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "115 | model.encoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "116 | model.encoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
            "117 | model.encoder.block.6.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "118 | model.encoder.block.6.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "119 | model.encoder.block.6.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "120 | model.encoder.block.6.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "121 | model.encoder.block.6.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "122 | model.encoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "123 | model.encoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
            "124 | model.encoder.block.7                                                 | T5Block                    | 7 M   \n",
            "125 | model.encoder.block.7.layer                                           | ModuleList                 | 7 M   \n",
            "126 | model.encoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "127 | model.encoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "128 | model.encoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "129 | model.encoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "130 | model.encoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "131 | model.encoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "132 | model.encoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "133 | model.encoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
            "134 | model.encoder.block.7.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "135 | model.encoder.block.7.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "136 | model.encoder.block.7.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "137 | model.encoder.block.7.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "138 | model.encoder.block.7.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "139 | model.encoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "140 | model.encoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
            "141 | model.encoder.block.8                                                 | T5Block                    | 7 M   \n",
            "142 | model.encoder.block.8.layer                                           | ModuleList                 | 7 M   \n",
            "143 | model.encoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "144 | model.encoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "145 | model.encoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "146 | model.encoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "147 | model.encoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "148 | model.encoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "149 | model.encoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "150 | model.encoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
            "151 | model.encoder.block.8.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "152 | model.encoder.block.8.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "153 | model.encoder.block.8.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "154 | model.encoder.block.8.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "155 | model.encoder.block.8.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "156 | model.encoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "157 | model.encoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
            "158 | model.encoder.block.9                                                 | T5Block                    | 7 M   \n",
            "159 | model.encoder.block.9.layer                                           | ModuleList                 | 7 M   \n",
            "160 | model.encoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "161 | model.encoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "162 | model.encoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "163 | model.encoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "164 | model.encoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "165 | model.encoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "166 | model.encoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "167 | model.encoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
            "168 | model.encoder.block.9.layer.1                                         | T5LayerFF                  | 4 M   \n",
            "169 | model.encoder.block.9.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "170 | model.encoder.block.9.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "171 | model.encoder.block.9.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "172 | model.encoder.block.9.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "173 | model.encoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "174 | model.encoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
            "175 | model.encoder.block.10                                                | T5Block                    | 7 M   \n",
            "176 | model.encoder.block.10.layer                                          | ModuleList                 | 7 M   \n",
            "177 | model.encoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
            "178 | model.encoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
            "179 | model.encoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
            "180 | model.encoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
            "181 | model.encoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
            "182 | model.encoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
            "183 | model.encoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
            "184 | model.encoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
            "185 | model.encoder.block.10.layer.1                                        | T5LayerFF                  | 4 M   \n",
            "186 | model.encoder.block.10.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
            "187 | model.encoder.block.10.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   \n",
            "188 | model.encoder.block.10.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   \n",
            "189 | model.encoder.block.10.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "190 | model.encoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
            "191 | model.encoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
            "192 | model.encoder.block.11                                                | T5Block                    | 7 M   \n",
            "193 | model.encoder.block.11.layer                                          | ModuleList                 | 7 M   \n",
            "194 | model.encoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
            "195 | model.encoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
            "196 | model.encoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
            "197 | model.encoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
            "198 | model.encoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
            "199 | model.encoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
            "200 | model.encoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
            "201 | model.encoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
            "202 | model.encoder.block.11.layer.1                                        | T5LayerFF                  | 4 M   \n",
            "203 | model.encoder.block.11.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
            "204 | model.encoder.block.11.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   \n",
            "205 | model.encoder.block.11.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   \n",
            "206 | model.encoder.block.11.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "207 | model.encoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
            "208 | model.encoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
            "209 | model.encoder.final_layer_norm                                        | T5LayerNorm                | 768   \n",
            "210 | model.encoder.dropout                                                 | Dropout                    | 0     \n",
            "211 | model.decoder                                                         | T5Stack                    | 137 M \n",
            "212 | model.decoder.block                                                   | ModuleList                 | 113 M \n",
            "213 | model.decoder.block.0                                                 | T5Block                    | 9 M   \n",
            "214 | model.decoder.block.0.layer                                           | ModuleList                 | 9 M   \n",
            "215 | model.decoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "216 | model.decoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "217 | model.decoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "218 | model.decoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "219 | model.decoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "220 | model.decoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "221 | model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   \n",
            "222 | model.decoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "223 | model.decoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
            "224 | model.decoder.block.0.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "225 | model.decoder.block.0.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "226 | model.decoder.block.0.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "227 | model.decoder.block.0.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "228 | model.decoder.block.0.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "229 | model.decoder.block.0.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "230 | model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias | Embedding                  | 384   \n",
            "231 | model.decoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "232 | model.decoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
            "233 | model.decoder.block.0.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "234 | model.decoder.block.0.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "235 | model.decoder.block.0.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "236 | model.decoder.block.0.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "237 | model.decoder.block.0.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "238 | model.decoder.block.0.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "239 | model.decoder.block.0.layer.2.dropout                                 | Dropout                    | 0     \n",
            "240 | model.decoder.block.1                                                 | T5Block                    | 9 M   \n",
            "241 | model.decoder.block.1.layer                                           | ModuleList                 | 9 M   \n",
            "242 | model.decoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "243 | model.decoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "244 | model.decoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "245 | model.decoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "246 | model.decoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "247 | model.decoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "248 | model.decoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "249 | model.decoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
            "250 | model.decoder.block.1.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "251 | model.decoder.block.1.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "252 | model.decoder.block.1.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "253 | model.decoder.block.1.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "254 | model.decoder.block.1.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "255 | model.decoder.block.1.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "256 | model.decoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "257 | model.decoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
            "258 | model.decoder.block.1.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "259 | model.decoder.block.1.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "260 | model.decoder.block.1.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "261 | model.decoder.block.1.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "262 | model.decoder.block.1.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "263 | model.decoder.block.1.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "264 | model.decoder.block.1.layer.2.dropout                                 | Dropout                    | 0     \n",
            "265 | model.decoder.block.2                                                 | T5Block                    | 9 M   \n",
            "266 | model.decoder.block.2.layer                                           | ModuleList                 | 9 M   \n",
            "267 | model.decoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "268 | model.decoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "269 | model.decoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "270 | model.decoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "271 | model.decoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "272 | model.decoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "273 | model.decoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "274 | model.decoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
            "275 | model.decoder.block.2.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "276 | model.decoder.block.2.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "277 | model.decoder.block.2.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "278 | model.decoder.block.2.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "279 | model.decoder.block.2.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "280 | model.decoder.block.2.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "281 | model.decoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "282 | model.decoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
            "283 | model.decoder.block.2.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "284 | model.decoder.block.2.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "285 | model.decoder.block.2.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "286 | model.decoder.block.2.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "287 | model.decoder.block.2.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "288 | model.decoder.block.2.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "289 | model.decoder.block.2.layer.2.dropout                                 | Dropout                    | 0     \n",
            "290 | model.decoder.block.3                                                 | T5Block                    | 9 M   \n",
            "291 | model.decoder.block.3.layer                                           | ModuleList                 | 9 M   \n",
            "292 | model.decoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "293 | model.decoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "294 | model.decoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "295 | model.decoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "296 | model.decoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "297 | model.decoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "298 | model.decoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "299 | model.decoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
            "300 | model.decoder.block.3.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "301 | model.decoder.block.3.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "302 | model.decoder.block.3.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "303 | model.decoder.block.3.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "304 | model.decoder.block.3.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "305 | model.decoder.block.3.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "306 | model.decoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "307 | model.decoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
            "308 | model.decoder.block.3.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "309 | model.decoder.block.3.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "310 | model.decoder.block.3.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "311 | model.decoder.block.3.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "312 | model.decoder.block.3.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "313 | model.decoder.block.3.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "314 | model.decoder.block.3.layer.2.dropout                                 | Dropout                    | 0     \n",
            "315 | model.decoder.block.4                                                 | T5Block                    | 9 M   \n",
            "316 | model.decoder.block.4.layer                                           | ModuleList                 | 9 M   \n",
            "317 | model.decoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "318 | model.decoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "319 | model.decoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "320 | model.decoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "321 | model.decoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "322 | model.decoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "323 | model.decoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "324 | model.decoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
            "325 | model.decoder.block.4.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "326 | model.decoder.block.4.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "327 | model.decoder.block.4.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "328 | model.decoder.block.4.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "329 | model.decoder.block.4.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "330 | model.decoder.block.4.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "331 | model.decoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "332 | model.decoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
            "333 | model.decoder.block.4.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "334 | model.decoder.block.4.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "335 | model.decoder.block.4.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "336 | model.decoder.block.4.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "337 | model.decoder.block.4.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "338 | model.decoder.block.4.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "339 | model.decoder.block.4.layer.2.dropout                                 | Dropout                    | 0     \n",
            "340 | model.decoder.block.5                                                 | T5Block                    | 9 M   \n",
            "341 | model.decoder.block.5.layer                                           | ModuleList                 | 9 M   \n",
            "342 | model.decoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "343 | model.decoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "344 | model.decoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "345 | model.decoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "346 | model.decoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "347 | model.decoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "348 | model.decoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "349 | model.decoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
            "350 | model.decoder.block.5.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "351 | model.decoder.block.5.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "352 | model.decoder.block.5.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "353 | model.decoder.block.5.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "354 | model.decoder.block.5.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "355 | model.decoder.block.5.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "356 | model.decoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "357 | model.decoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
            "358 | model.decoder.block.5.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "359 | model.decoder.block.5.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "360 | model.decoder.block.5.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "361 | model.decoder.block.5.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "362 | model.decoder.block.5.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "363 | model.decoder.block.5.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "364 | model.decoder.block.5.layer.2.dropout                                 | Dropout                    | 0     \n",
            "365 | model.decoder.block.6                                                 | T5Block                    | 9 M   \n",
            "366 | model.decoder.block.6.layer                                           | ModuleList                 | 9 M   \n",
            "367 | model.decoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "368 | model.decoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "369 | model.decoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "370 | model.decoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "371 | model.decoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "372 | model.decoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "373 | model.decoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "374 | model.decoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
            "375 | model.decoder.block.6.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "376 | model.decoder.block.6.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "377 | model.decoder.block.6.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "378 | model.decoder.block.6.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "379 | model.decoder.block.6.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "380 | model.decoder.block.6.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "381 | model.decoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "382 | model.decoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
            "383 | model.decoder.block.6.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "384 | model.decoder.block.6.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "385 | model.decoder.block.6.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "386 | model.decoder.block.6.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "387 | model.decoder.block.6.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "388 | model.decoder.block.6.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "389 | model.decoder.block.6.layer.2.dropout                                 | Dropout                    | 0     \n",
            "390 | model.decoder.block.7                                                 | T5Block                    | 9 M   \n",
            "391 | model.decoder.block.7.layer                                           | ModuleList                 | 9 M   \n",
            "392 | model.decoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "393 | model.decoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "394 | model.decoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "395 | model.decoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "396 | model.decoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "397 | model.decoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "398 | model.decoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "399 | model.decoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
            "400 | model.decoder.block.7.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "401 | model.decoder.block.7.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "402 | model.decoder.block.7.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "403 | model.decoder.block.7.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "404 | model.decoder.block.7.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "405 | model.decoder.block.7.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "406 | model.decoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "407 | model.decoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
            "408 | model.decoder.block.7.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "409 | model.decoder.block.7.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "410 | model.decoder.block.7.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "411 | model.decoder.block.7.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "412 | model.decoder.block.7.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "413 | model.decoder.block.7.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "414 | model.decoder.block.7.layer.2.dropout                                 | Dropout                    | 0     \n",
            "415 | model.decoder.block.8                                                 | T5Block                    | 9 M   \n",
            "416 | model.decoder.block.8.layer                                           | ModuleList                 | 9 M   \n",
            "417 | model.decoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "418 | model.decoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "419 | model.decoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "420 | model.decoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "421 | model.decoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "422 | model.decoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "423 | model.decoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "424 | model.decoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
            "425 | model.decoder.block.8.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "426 | model.decoder.block.8.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "427 | model.decoder.block.8.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "428 | model.decoder.block.8.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "429 | model.decoder.block.8.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "430 | model.decoder.block.8.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "431 | model.decoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "432 | model.decoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
            "433 | model.decoder.block.8.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "434 | model.decoder.block.8.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "435 | model.decoder.block.8.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "436 | model.decoder.block.8.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "437 | model.decoder.block.8.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "438 | model.decoder.block.8.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "439 | model.decoder.block.8.layer.2.dropout                                 | Dropout                    | 0     \n",
            "440 | model.decoder.block.9                                                 | T5Block                    | 9 M   \n",
            "441 | model.decoder.block.9.layer                                           | ModuleList                 | 9 M   \n",
            "442 | model.decoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
            "443 | model.decoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
            "444 | model.decoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
            "445 | model.decoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
            "446 | model.decoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
            "447 | model.decoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
            "448 | model.decoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
            "449 | model.decoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
            "450 | model.decoder.block.9.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
            "451 | model.decoder.block.9.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
            "452 | model.decoder.block.9.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
            "453 | model.decoder.block.9.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
            "454 | model.decoder.block.9.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
            "455 | model.decoder.block.9.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
            "456 | model.decoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
            "457 | model.decoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
            "458 | model.decoder.block.9.layer.2                                         | T5LayerFF                  | 4 M   \n",
            "459 | model.decoder.block.9.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
            "460 | model.decoder.block.9.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
            "461 | model.decoder.block.9.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
            "462 | model.decoder.block.9.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "463 | model.decoder.block.9.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
            "464 | model.decoder.block.9.layer.2.dropout                                 | Dropout                    | 0     \n",
            "465 | model.decoder.block.10                                                | T5Block                    | 9 M   \n",
            "466 | model.decoder.block.10.layer                                          | ModuleList                 | 9 M   \n",
            "467 | model.decoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
            "468 | model.decoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
            "469 | model.decoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
            "470 | model.decoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
            "471 | model.decoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
            "472 | model.decoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
            "473 | model.decoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
            "474 | model.decoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
            "475 | model.decoder.block.10.layer.1                                        | T5LayerCrossAttention      | 2 M   \n",
            "476 | model.decoder.block.10.layer.1.EncDecAttention                        | T5Attention                | 2 M   \n",
            "477 | model.decoder.block.10.layer.1.EncDecAttention.q                      | Linear                     | 589 K \n",
            "478 | model.decoder.block.10.layer.1.EncDecAttention.k                      | Linear                     | 589 K \n",
            "479 | model.decoder.block.10.layer.1.EncDecAttention.v                      | Linear                     | 589 K \n",
            "480 | model.decoder.block.10.layer.1.EncDecAttention.o                      | Linear                     | 589 K \n",
            "481 | model.decoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
            "482 | model.decoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
            "483 | model.decoder.block.10.layer.2                                        | T5LayerFF                  | 4 M   \n",
            "484 | model.decoder.block.10.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
            "485 | model.decoder.block.10.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   \n",
            "486 | model.decoder.block.10.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   \n",
            "487 | model.decoder.block.10.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "488 | model.decoder.block.10.layer.2.layer_norm                             | T5LayerNorm                | 768   \n",
            "489 | model.decoder.block.10.layer.2.dropout                                | Dropout                    | 0     \n",
            "490 | model.decoder.block.11                                                | T5Block                    | 9 M   \n",
            "491 | model.decoder.block.11.layer                                          | ModuleList                 | 9 M   \n",
            "492 | model.decoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
            "493 | model.decoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
            "494 | model.decoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
            "495 | model.decoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
            "496 | model.decoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
            "497 | model.decoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
            "498 | model.decoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
            "499 | model.decoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
            "500 | model.decoder.block.11.layer.1                                        | T5LayerCrossAttention      | 2 M   \n",
            "501 | model.decoder.block.11.layer.1.EncDecAttention                        | T5Attention                | 2 M   \n",
            "502 | model.decoder.block.11.layer.1.EncDecAttention.q                      | Linear                     | 589 K \n",
            "503 | model.decoder.block.11.layer.1.EncDecAttention.k                      | Linear                     | 589 K \n",
            "504 | model.decoder.block.11.layer.1.EncDecAttention.v                      | Linear                     | 589 K \n",
            "505 | model.decoder.block.11.layer.1.EncDecAttention.o                      | Linear                     | 589 K \n",
            "506 | model.decoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
            "507 | model.decoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
            "508 | model.decoder.block.11.layer.2                                        | T5LayerFF                  | 4 M   \n",
            "509 | model.decoder.block.11.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
            "510 | model.decoder.block.11.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   \n",
            "511 | model.decoder.block.11.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   \n",
            "512 | model.decoder.block.11.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "513 | model.decoder.block.11.layer.2.layer_norm                             | T5LayerNorm                | 768   \n",
            "514 | model.decoder.block.11.layer.2.dropout                                | Dropout                    | 0     \n",
            "515 | model.decoder.final_layer_norm                                        | T5LayerNorm                | 768   \n",
            "516 | model.decoder.dropout                                                 | Dropout                    | 0     \n",
            "517 | model.lm_head                                                         | Linear                     | 24 M  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0e0c9ce1a1a41eba222c5789119f7ca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layoutâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYeVDVjhvwSk"
      },
      "source": [
        "!mkdir t5_base_totto_pre_trained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLkTcYuqvwP6"
      },
      "source": [
        "## save the model this way so next time you can load it using T5ForConditionalGeneration.from_pretrained\n",
        "model.model.save_pretrained('t5_base_imdb_sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf7arZOVvwNK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEeaB1EKvwK9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199OUW5tvwG0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HFOmAi51yQt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlTgiBd11yOL"
      },
      "source": [
        "testDataLst = []\n",
        "\n",
        "with open('totto/unlabeled_totto_test_data.jsonl', 'r') as fd:\n",
        "  for l in fd:\n",
        "    testDataLst.append(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBffAFD3e9kD",
        "outputId": "72c84305-34ef-4e77-8963-ea0390a81cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "testDataLst[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"table\": [[{\"value\": \"nuclide symbol\", \"is_header\": true, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"Z(p)\", \"is_header\": true, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"N(n)\", \"is_header\": true, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"isotopic mass (u)\", \"is_header\": true, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"half-life\", \"is_header\": true, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"decay mode(s)\", \"is_header\": true, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"daughter isotope(s)\", \"is_header\": true, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"nuclear spin and parity\", \"is_header\": true, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"representative isotopic composition (mole fraction)\", \"is_header\": true, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"range of natural variation (mole fraction)\", \"is_header\": true, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"excitation energy\", \"is_header\": true, \"column_span\": 3, \"row_span\": 1}], [{\"value\": \"14F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"14.03432(4)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"500(60)\\\\u00d710\\\\u221224 s [910 keV]\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"p\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"13O\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"2-\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"15F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"6\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"15.017785(15)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"1.1(0.3)\\\\u00d710\\\\u221221 s [1.0(2) MeV]\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"p\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"14O\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"1/2+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"16F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"7\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"16.011466(9)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"11(6)\\\\u00d710\\\\u221221 s [40(20) keV]\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"p\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"15O\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"0\\\\u2212\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"17F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"8\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"17.00209524(27)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"64.370(27) s\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\\\\u03b2+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"17O\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5/2+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"18F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"18.0009373(5)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"109.739(9) min\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2+ (96.86%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"18O\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"1+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"EC (3.14%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"18O\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"18mF\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"1121.36(15) keV\", \"is_header\": false, \"column_span\": 3, \"row_span\": 1}, {\"value\": \"162(7) ns\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"IT\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"18F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"19F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"10\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"18.9984031629(9)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"Stable\", \"is_header\": false, \"column_span\": 3, \"row_span\": 1}, {\"value\": \"1/2+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"1.0000\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"20F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"11\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"19.99998125(3)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"11.163(8) s\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\\\\u03b2\\\\u2212\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"20Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"2+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"21F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"12\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"20.9999489(19)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"4.158(20) s\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\\\\u03b2\\\\u2212\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"21Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5/2+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"22F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"13\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"22.002999(13)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"4.23(4) s\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2\\\\u2212 (89%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"22Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"(4+)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"\\\\u03b2\\\\u2212n (11%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"21Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"23F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"14\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"23.00353(4)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"2.23(14) s\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2\\\\u2212 (86%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"23Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5/2+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"\\\\u03b2\\\\u2212n (14%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"22Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"24F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"15\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"24.00810(10)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"384(16) ms\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2\\\\u2212 (94.1%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"24Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"3+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"\\\\u03b2\\\\u2212n (5.9%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"23Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"25F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"16\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"25.01217(10)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"80(9) ms\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2\\\\u2212 (76.9%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"25Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"(5/2+)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"\\\\u03b2\\\\u2212n (23.1%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"24Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"26F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"17\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"26.02002(12)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"8.2(9) ms\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2\\\\u2212 (86.5%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"26Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"1+\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"\\\\u03b2\\\\u2212n (13.5%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"25Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"26mF\", \"is_header\": false, \"column_span\": 1, \"row_span\": 3}, {\"value\": \"643.4(1) keV\", \"is_header\": false, \"column_span\": 3, \"row_span\": 3}, {\"value\": \"2.2(1) ms\", \"is_header\": false, \"column_span\": 1, \"row_span\": 3}, {\"value\": \"IT (82%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"26F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"(4+)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 3}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 3}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 3}], [{\"value\": \"\\\\u03b2\\\\u2212n (12%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"25Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"\\\\u03b2\\\\u2212n (6%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"26Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"27F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"18\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"27.02732(42)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"4.9(2) ms\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2\\\\u2212, n (77%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"26Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5/2+#\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"\\\\u03b2\\\\u2212 (23%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"27Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"28F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"19\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"28.03622(42)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"46\\\\u00d710-21 s\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"n\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"27F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"29F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"20\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"29.04310(56)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"2.5(3) ms\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\\\\u03b2\\\\u2212, n (60%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"28Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5/2+#\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 2}], [{\"value\": \"\\\\u03b2\\\\u2212 (40%)\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"29Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}], [{\"value\": \"31F\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"9\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"22\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"31.06027(59)#\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"1# ms [>260 ns]\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\\\\u03b2\\\\u2212\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"31Ne\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"5/2+#\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}, {\"value\": \"\", \"is_header\": false, \"column_span\": 1, \"row_span\": 1}]], \"table_webpage_url\": \"http://en.wikipedia.org/wiki/Isotopes_of_fluorine\", \"table_page_title\": \"Isotopes of fluorine\", \"table_section_title\": \"List of isotopes\", \"table_section_text\": \"Bold for stable isotopes Has medicinal uses\", \"highlighted_cells\": [[8, 0], [8, 2]], \"example_id\": -108316152648214282, \"overlap_subset\": true}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQWyTDTW8S90"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}